# -*- after-save-hook: org-latex-export-to-pdf; org-export-in-background: t; -*-
#+TITLE:Solutions to Homework 4
#+AUTHOR:Darlan Cavalcante Moreira
#+EMAIL:darcamo@gmail.com
#+SETUPFILE: ~/.emacs.d/lisp/org-setup-file-template/setup.org
#+options: num:nil
#+property: header-args:python :exports results
#+HTML_MATHJAX: path:MathJax-master/MathJax.js?config=default.js  align: left indent: 5em tagside: left font: Neo-Euler

#+LATEX_HEADER: \renewcommand{\theenumi}{\alph{enumi}}
#+LATEX_HEADER: \usepackage[paper=a4paper]{geometry}
#+HTML_HEAD: <style>ol {list-style-type: lower-latin;}</style>
#+LATEX: \newcommand\detR{\vert a\vert^4+\vert b\vert^4+\vert a\vert^2 \vert b\vert^2}


* Exercise 1
  a. 
     \begin{equation*}
     \vtU[n] = \begin{bmatrix} u[n] \\ u[n-1] \\ u[n-2] \\ \vdots \\ u[n-M+1] \\ \end{bmatrix}  \quad \quad \vtW = \begin{bmatrix} w[0] \\ w[1] \\ w[2] \\ \vdots \\ w[M-1]\end{bmatrix}\end{equation*}
     
     \begin{equation*}
     \begin{aligned}y[n] & = \sum_{m=0}^{M-1} w^*[m] \cdot u[n-m] \\ & = \vtW^H \vtU[n]\end{aligned}
     \end{equation*}
  b. 
     \begin{equation*}
     \begin{aligned}
     \text{LMMSE:\quad}  & \min \; \E\{\Vert y[n]-d[n]\Vert^2\} \\
     & \min \; \E\{\Vert \vtW^H \vtU[n] - d[n] \Vert^2\}
     \end{aligned}
     \end{equation*}

     The expectation can be computed as 
     \begin{equation*}
     \begin{aligned}
     \E\{y-d\} & = \E\{(\vtW^H \vtU[n] - d[n]) (\vtW^H \vtU[n] - d[n] )^H\} \\
     & = \E\{\vtW^H \vtU[n]\vtU^H[n] \vtW - \vtW^H\vtU[n]d^*[n] - \underbrace{\vtU^H[n]\vtW d[n]}_{\vtW^T\vtU^*[n]d[n]} + d^2[n]\} \\
     & = \vtW\mtR_u\vtW - \vtW^H\vtP - \vtW^T \vtP^* + \mtR_d
     \end{aligned}
     \end{equation*}
     where $\vtP = \E\{\vtU[n]d^*[n]\}$, $\mtR_u = \E\{\vtU[n] \vtU^H[n]\}$ and $\mtR_d$ is an identity matrix.

     In order to find the optimum $\vtW$ we derivate this with respect to
     $\vtW^*$ resulting in
     
     \begin{equation*}
     \begin{aligned}
     \vtR_u\vtW - \vtP = 0 \\
     \boxed{\vtW = \mtR^{-1}\vtP}
     \end{aligned}
     \end{equation*}
  c. 
     \begin{equation*}
     \begin{aligned}
     r[m] & = \E\{u[n]u^*[n-m]\} \\ u[n] &= \vtH^T \vtX[n] \\ \text{with} & \quad \vtH = \begin{bmatrix} h[0]\\h[1]\\\vdots\\ h[L_h-1]\end{bmatrix} \quad \text{and} \quad \vtX[n] = \begin{bmatrix} x[n]\\x[n-1]\\\vdots \\x[n-L_h+1]\end{bmatrix}
     \end{aligned}
     \end{equation*}

     The covariance matrix $\mtR$ has the form below

     $$ \begin{bmatrix} r[0] & r[1] & \cdots & r[M-2] & r[M-1] \\ r[-1] & r[0] & r[1] & \cdots \\ r[-2] & r[-1] & r[0] & \cdots \\ \vdots & \vdots & & \ddots & \vdots \\ r[-M+1] & r[-M+2] & \cdots & &r[0]\end{bmatrix} $$

     Each element in this matrix follows the form

     \begin{equation*}
     \begin{aligned}
     r[m] & = \E\{u[n]u^*[n-m]\} = \E\{(\vtH^T\vtX[n])(\vtX[n-m])^H\} \\
     & = \vtH^T \underbrace{\E\{\vtX[n]\vtX^H[n-m]\}}_{\mtX[m]}\vtH^*
     \end{aligned}
     \end{equation*}

     $\mtX[m]$ is a matrix with dimension $L_h \times L_h$. Considering that
     $\E\{x[n]x^*[n-k]\}$ is equal to 1 for $k=0$ and $0$ otherwise, let's compute
     the value of $\mtX[m]$ for some values of $m$.  for $\mtX[m]$ for some value
     of $m$.

     \begin{equation*}
     \mtX[0] = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 \\ \vdots & & \ddots \\ 0 & 0 & \cdots & 1\end{bmatrix} \quad \rightarrow \quad \text{Identity Matrix}
     \end{equation*}

     \begin{equation*}
     \mtX[1] = \begin{bmatrix} 0 & 0 & 0 & \cdots \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ \vdots & & \ddots \\ 0 & 0 & \cdots & 1\end{bmatrix} \quad \rightarrow \quad \text{Diagonal of ones shifted below}
     \end{equation*}

     All $\mtX[m]$ matrices correspond to an identity matrix with its diagonal shifted below by $m$.

     Applying this back to $r[m]$ we get
     \begin{equation*}
     \begin{aligned}
     r[0] = \vtH^T \vtH = \sum_{k=0}^{L_h-1}h[k]h^*[k] = \Vert \vtH\Vert^2\\
     r[1] = \sum_{k=0}^{L_h-1}h[k]h^*[k-1] \\
     r[2] = \sum_{k=0}^{L_h-1}h[k]h^*[k-2] \\
     \vdots \\
     \end{aligned}
     \end{equation*}

     Therefore, the general formula for $r[m]$ is given by

     \begin{equation*}
     \boxed{r[m] = \sum_{k=0}^{L_h-1}h[k]h^*[k-m]}
     \end{equation*}
  
     For the value of $\vtP$ we have that

     \begin{equation*}
     \begin{aligned}
     \vtP & = \E\{\vtU[n] x^*[n-M+1]\} = \begin{bmatrix} \E\{u[n] x^*[n-M+1]\} \\ \E\{u[n-1] x^*[n-M+1]\} \\ \vdots \\ \E\{u[n-M+1] x^*[n-M+1]\}\end{bmatrix} \\
     & = \begin{bmatrix} \displaystyle \E \left\{ \left(\sum_{k=0}^{L_h-1}h[k]x[n-k] \right)   x^*[n-M+1]   \right\} \\ \displaystyle \E \left\{ \left(\sum_{k=0}^{L_h-1}h[k]x[n-k-1] \right)   x^*[n-M+1]   \right\} \\ \vdots \\ \displaystyle \E \left\{ \left(\sum_{k=0}^{L_h-1}h[k]x[n-k-M+1] \right)   x^*[n-M+1]   \right\}\end{bmatrix} \\
     & = \begin{bmatrix} \displaystyle \sum_{k=0}^{L_h-1} \E\left\{h[k]x[n-k] x^*[n-M+1] \right\} \\ \displaystyle \sum_{k=0}^{L_h-1} \E\left\{h[k]x[n-k-1] x^*[n-M+1] \right\} \\ \vdots \\ \displaystyle \sum_{k=0}^{L_h-1} \E\left\{h[k]x[n-k-M+1] x^*[n-M+1] \right\}\end{bmatrix}
     \end{aligned}
     \end{equation*}

     Due to the expectation only one term in each summation will be equal to 1
     times the corresponding channel tap and all other terms will be equal to
     zero. We have then that
     $$\vtP = \begin{bmatrix} h[M-1] \\ h[M-2] \\ \vdots \\ h[0]\end{bmatrix}$$

  d. 
     The Winer solution is given by $\vtW = \mtR^{-1}\vtP$ where 
     \(\vtH = \begin{bmatrix} a & b \end{bmatrix}^T\), 
     \(\vtP = \begin{bmatrix} b & a \end{bmatrix}\) and
     \begin{equation*}
     \mtR = \begin{bmatrix}
     r[0] & r[1] \\
     r[-1] & r[0]
     \end{bmatrix} = 
     \begin{bmatrix} 
     aa^*+bb^* & ba^* \\ ab^* & aa^*+bb^*
     \end{bmatrix} = 
     \begin{bmatrix}
     a^2+b^2 & ba^* \\ ab^* & a^2+b^2
     \end{bmatrix}
     \end{equation*}

     In order to compite the inverse of $\mtR$ let's first compute its
     determinant, which is given by $\det(\mtR) = \vert a \vert^4 + \vert b
     \vert^4 + \vert ab \vert^2$.

     Therefore, the inverse ot $\mtR$ is given by

     \begin{equation*}
     \mtR^{-1} = \begin{bmatrix}
     \frac{\vert a\vert^2+\vert b\vert^2}{\detR} & \frac{-ba^*}{\detR} \\ \frac{-ab^*}{\detR} & \frac{\vert a\vert^2+\vert b\vert^2}{\detR}
     \end{bmatrix}
     \end{equation*}
     
     Therefore, the Winer solution is given by
     
     \begin{equation*}
     \begin{aligned}
     \vtW & = \mtR^{-1}\vtP = \begin{bmatrix} \frac{\vert a\vert^2+\vert b\vert^2}{\detR} & \frac{-ba^*}{\detR} \\ \frac{-ab^*}{\detR} & \frac{\vert a\vert^2+\vert b\vert^2}{\detR} \end{bmatrix} \begin{bmatrix} b\\a \end{bmatrix} \\
     \vtW & = \frac{1}{\detR} \begin{bmatrix} (\vert a\vert^2+\vert b\vert^2)b - b\vert a \vert^2 \\ -a\vert b \vert^2 + (\vert a\vert^2+\vert b\vert^2)a\end{bmatrix} \\
     \vtW & = \frac{1}{\detR} \begin{bmatrix} \vert b \vert^2 b \\ \vert a \vert^2 a \end{bmatrix}
     \end{aligned}
     \end{equation*}

     If we set $b=0$ then $M=1$ and we have that \(w = \frac{a}{\vert a \vert^2}\) and the error is given by
     \[e[n] = d[n] - y[n] = x[n - 1 + 1] - \frac{a^*}{\vert a \vert^2} (a x[n]) = 0.\]

  e. 
     Now we have that \(\vtH = \begin{bmatrix} 4 & -5\end{bmatrix}^T\) and
     since \(M = 6\) the covariance matrix is a \(6 \times 6\) matrix given by

     \begin{equation}
     \mtR = \begin{bmatrix}41 & -20 & 0 & 0 & 0 & 0\\-20 & 41 & -20 & 0 & 0 & 0\\0 & -20 & 41 & -20 & 0 & 0\\0 & 0 & -20 & 41 & -20 & 0\\0 & 0 & 0 & -20 & 41 & -20\\0 & 0 & 0 & 0 & -20 & 41\end{bmatrix}
     \end{equation}

     with the winer solution given by

     \begin{equation*}
     \vtW = \left[\begin{matrix}-0.0308479052192436\\-0.0632382056994494\\-0.0987904164646277\\-0.139282148053037\\-0.186737987044099\\0.00646927461263472\end{matrix}\right]
     \end{equation*}

     Using this filter and simulating for 50000 random samples of a zero mean
     circularly complex gaussian distribution with unitary variance resulted in
     a resudual mean square error of 0.040465415688231844.

  f. 
     The plot of the MSE is shown below. 

     [[./mse_for_varying_M.png]]

     It is possible to see a "linear" decreasing
     (considering the logarithmic scale for the y axis). Different from the
     last item, where 50000 data points were simulated (more than enough), in
     this item I have increased the number of data points to 10000000 to
     improve the averaging of the MSE. Even that number is not enough for the
     higher values of M, which explains the "jumping around" of the plot in the
     higher values of M.

     
* Exercise 2
  a. 
  b. 
